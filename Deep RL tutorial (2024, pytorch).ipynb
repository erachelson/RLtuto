{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://erachelson.github.io/RLtuto/\">https://erachelson.github.io/RLtuto/</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Deep RL tutorial<br> -<br> Deep RL with (almost) no maths</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few words about the author:**  \n",
    "I am a Reinforcement Learning researcher with a few other scientific interests (Operations Research, Planning, Machine Learning). I defended my PhD in 2008 and my habilitation in 2020, and have occupied both academic and industry positions in research, before joining ISAE-SUPAERO in 2011 as an Associate Professor, then a Full Professor. I created the [Data and Decision Sciences](https://supaerodatascience.github.io) Master-level curriculum, which is now running with the help of an incredible team of speakers and professors, for even more incredible students. I kickstarted the Artificial Intelligence and Business Transformation master program. I also founded [SuReLI](https://sureli.github.io), ISAE-SUPAERO's Reinforcement Learning research Initiative. I currently hold the chair of Reinforcement Learning within [ANITI](https://aniti.univ-toulouse.fr/en/).\n",
    "\n",
    "**Abstract**  \n",
    "This tutorial aims at introducing the key concepts in (Deep) Reinforcement Learning in a practical, intuitive way, for people who don't know Reinforcement Learning. It keeps the jargon, technicalities and theory as limited as possible in order to leave room for algorithms and individual practice.  \n",
    "It supposes the reader has a minimum knowledge of Python and a few basic math (probability and optimization) notions.\n",
    "\n",
    "**References:**  \n",
    "Readers interested in the field are encouraged to discover more by consulting the following references.\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"img/book_szepesvari.jpg\" style=\"width: 100px;\"></td>\n",
    "<td>\n",
    "    \n",
    "**Algorithms for Reinforcement Learning**<br>Csaba Szepesvari<br>2010.<br>The essentials in 104 pages. A bit mathematical.<br>PDF available <a href=\"https://sites.ualberta.ca/~szepesva/RLBook.html\">here</a> (last update in 2010).</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"img/book_sutton.jpg\" style=\"width: 100px;\"></td>\n",
    "<td>\n",
    "    \n",
    "**Reinforcement Learning: an introduction**<br>Richard Sutton and Andrew Barto<br>2018 (second edition).<br>The Reinforcement Learning bible. Both complete and didactical.<br><a href=\"http://incompleteideas.net/book/the-book.html\">Online versions</a> of the <a href=\"http://incompleteideas.net/book/the-book-1st.html\">1st</a> and <a href=\"http://incompleteideas.net/book/the-book-2nd.html\">2nd</a> editions.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"img/web_silver.png\" style=\"width: 100px;\"></td>\n",
    "<td>\n",
    "    \n",
    "**David Silver's UCL course on RL**<br>10 video lectures + presentation PDFs.<br>2015.<br><a href=\"http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\">Available here</a>.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><img src=\"img/web_rlvs.png\" style=\"width: 100px;\"></td>\n",
    "<td>\n",
    "    \n",
    "**The Reinforcement Learning Virtual School**<br>20+ online classes, from RL basics to state-of-the-art topics.<br>2021.<br><a href=\"https://rl-vs.github.io/rlvs2021\">Available here</a>.</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "**Installation**  \n",
    "`pip install torch gymnasium gymnasium[atari,accept-rom-licence,other]`\n",
    "\n",
    "**Table of contents**\n",
    "1. [What is Reinforcement Learning ?](#sec1)\n",
    "2. [Let's have fun, let's learn to play Breakout](#sec2)\n",
    "3. [Moving towards real-life problems](#sec3)\n",
    "4. [A bit of theory](#sec4)\n",
    "5. [Your turn to play](#sec5)\n",
    "6. [Wrapping up and going further](#sec6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec1\"></a> What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning is the discipline that studies how one can learn to control a system through interaction.\n",
    "\n",
    "<center><img src=\"img/juggle.jpg\" width=\"400px\"></center>\n",
    "\n",
    "- No model\n",
    "- The *learning* version of Stochastic Optimal Control.\n",
    "- Applies to industrial scheduling, robotics control, Go playing...\n",
    "\n",
    "<div class=\"alert alert-success\"> \n",
    "    \n",
    "**The goal in RL**<br>\n",
    "Construct a close-loop control policy that maximizes a certain criterion, based on interaction data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec2\"></a> Let's have fun, let's learn to play Breakout\n",
    "\n",
    "## Breakout\n",
    "\n",
    "Breakout is an old Atari 2600 game. Let's play a little with its Gymnasium environment.\n",
    "\n",
    "Gymnasium is a collection of sequential decision making environments. It is designed to develop and compare RL algorithms and provides a standard API across environments.\n",
    "- [https://gymnasium.farama.org](https://gymnasium.farama.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "breakout = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\")\n",
    "breakout.metadata['render_fps'] = 60\n",
    "print(\"Observation space:\", breakout.observation_space)\n",
    "print(\"Action space:\", breakout.action_space)\n",
    "s,_ = breakout.reset()\n",
    "plt.imshow(breakout.render(),vmin=0,vmax=255);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "breakout = gym.make('Breakout-v4', render_mode=\"rgb_array_list\")\n",
    "breakout.metadata['render_fps'] = 20\n",
    "\n",
    "for _ in range(3):\n",
    "    breakout.reset()\n",
    "    breakout.step(1)\n",
    "    breakout.render()\n",
    "    for i in range(100):\n",
    "        a = 2+np.random.randint(2)\n",
    "        observation, reward, done, trunc, info = breakout.step(a)\n",
    "        #breakout.render()\n",
    "        #time.sleep(1./60.)\n",
    "        if done:\n",
    "            print(\"game over\") # when will this game stop?\n",
    "\n",
    "save_video(breakout.render(), \"videos\", fps=breakout.metadata[\"render_fps\"], name_prefix=\"random_policy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"videos/random_policy-episode-0.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout.close()\n",
    "breakout = gym.make(\"Breakout-v4\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4 available actions in Breakout are as follows:,\n",
    "- 0 NOOP (no operation),\n",
    "- 1 FIRE (press fire button),\n",
    "- 2 RIGHT (move paddle right),\n",
    "- 3 LEFT (move paddle left),\n",
    "\n",
    "Let's observe the output of an `env.step(a)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakout.reset()\n",
    "s, r, d, trunc, info = breakout.step(1)\n",
    "print(s.shape)\n",
    "print(r)\n",
    "print(d)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we press a button, the `step` function returns:\n",
    "- the next screen, \n",
    "- a \"reward\" signal indicating how much we've won during this time step,\n",
    "- a boolean indicating if we've lost,\n",
    "- another boolean indicating if the episode was artificially truncated,\n",
    "- some extra information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's pretty close to the information a human player would have, time step after time step (no ball coordinates, no velocities, etc. just the raw RGB image).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Let's discuss for a minute how you believe a human player learns to play Breakout.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few other environments\n",
    "\n",
    "FrozenLake is a small, text-rendering-based, toy environment, where you want to navigate across a slippery map (try `help(fl.FrozenLakeEnv)` below for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import matplotlib.pyplot as plt\n",
    "lake = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "lake.reset()\n",
    "plt.imshow(lake.render());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole is a classic non-linear control problem, where you want to keep a pole upright and prevent it from falling down (quite similarly to a famous two-wheeled, self-balancing personal transporter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.utils.save_video import save_video\n",
    "\n",
    "cartpole = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "\n",
    "print(cartpole.action_space)\n",
    "print(cartpole.observation_space)\n",
    "print(cartpole.env.metadata)\n",
    "\n",
    "import time\n",
    "\n",
    "x,_ = cartpole.reset()\n",
    "cartpole.render()\n",
    "for i in range(200):\n",
    "    _, _, d, _, _ = cartpole.step(np.random.randint(2))\n",
    "    cartpole.render()\n",
    "    if d:\n",
    "        print(\"Episode ended at time step\", i)\n",
    "        #break\n",
    "        cartpole.reset()\n",
    "    time.sleep(cartpole.unwrapped.tau) # not actually real-time, just for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These environments are examples which we will reuse later on in the class. For now, let's get back to playing Breakout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "We are going to replicate the results that were introduced in the crucial paper **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** by Mnih et al. (2013) and were later improved by DeepMind's paper in Nature **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** by Mnih et al. (2015).\n",
    "\n",
    "We are going to need a few pre-requisites that we will introduce with as little theory as possible.\n",
    "\n",
    "We are going to implement Q-learning, the most classic RL algorithm. To do so, we need to introduce the notion of Q-values (we will call it Q-value of Q-function indistinctively).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Q-values**<br>\n",
    "The Q-value of an \"image, action\" pair indicates how much we can expect to score if we start by performing this action, from the current image. So $Q(image,action)$ ranks actions in a given state of the game. For simplicity, we will write it $Q(s,a)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we know the best possible $Q$-function, we have actually solved our problem since we only need to act greedily with respect to this function. The goal of Q-learning is precisely to learn this optimal Q-function that we will write $Q^*$ (more details on this in the theory part).\n",
    "\n",
    "Suppose we have a current estimate $Q$ for the optimal Q-function and we have just played $a$ in state $s$. That is, we have observed the $(s,a,r,s')$ transition, where:\n",
    "- $s$ is the starting state (image)\n",
    "- $a$ is the action performed\n",
    "- $r$ is the transition's observed reward\n",
    "- $s'$ is the reached state (image)\n",
    "\n",
    "Then one can remark that the sum $r + \\max_{a'} Q(s',a')$ gives an estimate of how much we can get in the long run. Of course, if $Q$ is close to $Q^*$, then this estimate is good. Otherwise, this estimate is poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discriminate between a reward obtained now and another obtained in one time step, we introduce a *discount* factor $\\gamma$ on future rewards. So the overall score estimate is $r + \\gamma \\max_{a'} Q(s',a')$\n",
    "\n",
    "We can compare this estimate $r + \\gamma \\max_{a'} Q(s',a')$ to what we actually believed was the value of the $(s,a)$ pair *before* we tried action $a$ and compute a correction term that we will call a temporal difference:\n",
    "\n",
    "$$\\delta=r+\\gamma \\max_{a'} Q(s',a') - Q(s,a)$$\n",
    "\n",
    "The temporal difference (TD in short) measures the difference in the estimate of $Q^*$ before and after obtaining the sample $(s,a,r,s')$. Ideally, we want to \"pull\" the estimate of $Q^*(s,a)$ towards $r+\\gamma \\max_{a'} Q(s',a')$. So that means pulling it in the direction of the temporal difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this yields a straightforward algorithm:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Q-learning**<br>\n",
    "Repeat:\n",
    "<ol>\n",
    "<li> In $s$, choose $a$\n",
    "<li> Observe $r$, $s'$\n",
    "<li> Temporal difference: $\\delta=r+\\gamma \\max_{a'} Q(s',a') - Q(s,a)$\n",
    "<li> Update $Q$: $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta$\n",
    "<li> $s\\leftarrow s'$\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Brainstorming**<br>\n",
    "What conditions do you think we should impose for this procedure to work?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "<ul>\n",
    "<li> \n",
    "    \n",
    "We need to obtain samples for every $(s,a)$ pair so the action selection procedure should **explore**. That means every $(s,a)$ pair should be visited often enough to let $Q(s,a)$ converge. But we're more interested in fast convergence in state-action pairs that yield high scores, so we would also like to focus on them and thus **exploit** already acquired knowledge. This is called the **exploration versus exploitation tradeoff**.\n",
    "<li> \n",
    "    \n",
    "Writing $Q(s,a) \\leftarrow Q(s,a) + \\alpha \\delta$  means that for every experience sample $(s,a,r,s')$ we take a small step towards a (hopefully) better estimate of $Q^*$. This is a well-known problem in optimization called *stochastic approximation* and popularized in recent years under the name of *Stochastic Gradient Descent* (SGD). SGD is guaranteed to converge under certain conditions on the step size $\\alpha$, which we will study in the theoretical part.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks and Q-learning\n",
    "\n",
    "An issue with the writing above is that, to obtain the true optimal Q-function, we need to collect enough representative samples in all states $s$ and all actions $a$. Let's make a quick calculation, there are:\n",
    "- $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$ possible images\n",
    "- 4 actions\n",
    "\n",
    "So that's really a lot of values.\n",
    "\n",
    "But one can remark two things:\n",
    "- the images that will actually be seen in practice are way less numerous\n",
    "- two \"close\" images are likely to have \"close\" optimal Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So we would like to learn an approximation function that maps state-action pairs to a scalar value, that we can train incrementally. Neural networks are just perfect for that task and deep neural networks have been the key to impressive results in the last decade. Additionnaly, our state is an image so that's even better: we can use convolutional neural networks (CNN) that learn functions over images and are at the core of the Deep Learning revolution.\n",
    "\n",
    "<center><img src=\"img/dqlas.png\" height=\"15%\" width=\"15%\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's modify slightly the previous algorithm to include a neural network $f$ as a Q-function approximator.\n",
    "\n",
    "<blockquote>\n",
    "Repeat:\n",
    "<ol>\n",
    "<li> In $s$, choose $a$\n",
    "<li> Observe $r$, $s'$\n",
    "<li> Define $x=(s,a)$ and $y=r+\\gamma \\max_{a'} f(s',a')$\n",
    "<li> $f$.train(x,y)\n",
    "<li> $s\\leftarrow s'$\n",
    "</ol>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State definition\n",
    "\n",
    "If we define the game's state to hold only the last frame, it might not be enough to define an informative enough state. For instance, having just a snapshot image only gives us the position of the ball but not its velocity. So we will expand the state so that it contains the 4 last frames from the game.\n",
    "\n",
    "One frame is a $210\\times 160$ RGB image with a 256 color palette, so the set of all possible frames has size $256^{210 \\times 160 \\times 3} \\sim 10^{242579}$. That's a little too many for an efficient enumeration. Even by converting the image to greyscale, downsampling to a $110\\times 84$ and then cropping to an $84\\times 84$ image to keep only the playing area (as we shall do a little later to slightly simplify the problem), that's still around $10^{16980}$ possible states. So, definitely, this discrete problem is not suitable for complete enumeration.\n",
    "\n",
    "Of course, most of the possible images will never occur in a Breakout game and the true state space is actually a much smaller subset of the full set of possible images. Nevertheless, unless we provide a large engineering effort in describing the state space with few variables (which would be contradictory of our goal of a \"human-level\" AI) we will need to automatically discover some structure in the state sampled data, hence justifying our usage of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "def process_screen(x):\n",
    "    return 256*resize(rgb2gray(x), (110,84))[17:101,:]\n",
    "\n",
    "x, _ = breakout.reset()\n",
    "y = process_screen(x)\n",
    "plt.imshow(y, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack the 4 last frames\n",
    "z = np.stack([y,y,y,y],axis=0)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice thing about Gymnasium is that it provides lots of wrappers around the base environments. For example, the following wrapper does almost all the previous operations (greyscale, subsampling, cropping, frame stacking). This wrapper actually only stores each frame once which optimizes memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "breakout = gym.make(\"PongNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "breakout = AtariPreprocessing(breakout)\n",
    "breakout = FrameStack(breakout, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = breakout.reset()\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "plt.imshow(x[0], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A specific neural network structure\n",
    "\n",
    "Let's define a Convolutional Neural Network (CNN) that will predict our Q-values.  \n",
    "When we choose a greedy action, we need to evaluate $Q(s,a)$ for all possible $a$. So in our case, that means propagating $(s,a)$ values 4 times through our network. It would be more efficient to build a network that predicts the values of the 4 actions in a single pass for a given state $s$. So we define the input of our Q-network as the state only, and the output as the 4-dimensional vector evaluating each action.\n",
    "\n",
    "<center><img src=\"img/dqls.png\" height=\"30%\" width=\"30%\"></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, our network has the following structure:\n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 16 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 32 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Fully connected with 256 ReLU units\n",
    "- layer 4 (output): Fully connected with 4 linear units (one for each action's value)\n",
    "\n",
    "Graphically, this yields the following network structure.\n",
    "\n",
    "<center><img src=\"img/dqn_keras.png\"></img></center>\n",
    "\n",
    "We refer to this type of CNN as *Deep Q-Networks* (DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class BreakoutCNN(nn.Module):\n",
    "    def __init__(self, in_channels=4, n_actions=4):\n",
    "        super(BreakoutCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        self.fc = nn.Linear(9 * 9 * 32, 256)\n",
    "        self.head = nn.Linear(256, n_actions)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.fc(x.view(x.size(0), -1)))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN above is generally sufficient to start learning good strategies for Atari games. It was introduced in the 2013 classical **[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)** paper. However, the \"canonical\" architecture for playing Atari is rather the one proposed in the **[Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning)** paper in 2015. The latter uses this architecture:\n",
    "- input: $84\\times 84\\times 4$ image (the last 4 frames)\n",
    "- layer 1: Convolutions with 32 filters of size $8\\times 8$ and stride 4. The activation is a ReLU function.\n",
    "- layer 2: Convolutions with 64 filters of size $4\\times 4$ and stride 2. The activation is a ReLU function.\n",
    "- layer 3: Convolutions with 64 filters of size $3\\times 3$ and stride 1. The activation is a ReLU function.\n",
    "- layer 4: Fully connected with 512 ReLU units\n",
    "- layer 5 (output): Fully connected with 4 linear units (one for each action's value)\n",
    "\n",
    "It's been a while since you have done something yourself in this class. Let's be a bit active and warm-up.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:** Change the code above to create this DQN.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "Neural networks learn better when the samples are fed to them in *mini-batches* rather than one by one. To implement this, our algorithm must remember the mapping $s,a\\mapsto r+\\gamma\\max_{a'}Q(s',a')$ for all the $(s,a)$ in a given mini-batch. So that means keeping a memory of the $(s,a,r,s')$ samples.\n",
    "\n",
    "But if we're going to store the $(s,a,r,s')$ samples, we might as well use each of them several times, to build increasingly better values for $r+\\gamma\\max_{a'}Q(s',a')$, as $Q$ gets more accurate (close to $Q^*$). This leads to the idea of *Experience Replay*: we store a *replay memory* of experience samples, among which we randomly draw elements that will constitute our mini-batch.\n",
    "\n",
    "Given this last idea, lets implement an Experience Replay Q-learning using our DQN. The algorithm's pseudo-code is:\n",
    "\n",
    "         state = init()\n",
    "         loop:\n",
    "            action = greedy_action(network) or random_action()\n",
    "            new_state, reward = step(state, action)\n",
    "            replay_memory.add(state, action, reward, new_state)\n",
    "            minibatch = replay_memory.sample(minibatch_size)\n",
    "            X_train = Y_train = []\n",
    "            for (s,a,r,s') in minibatch:\n",
    "                Q  = network.predict(s)\n",
    "                Q' = network.predict(s')\n",
    "                if non-terminal(s'): \n",
    "                    update = r + gamma * max(Q')    \n",
    "                else:  \n",
    "                    update = r\n",
    "                Q[a] = update\n",
    "                X_train.add(s)\n",
    "                Y_train.add(Q)\n",
    "            network.train_one_step(X_train,Y_train)\n",
    "            state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a class for our replay memory.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:** Fill the missing code lines below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    A replay memory. Implements a fixed size circular (FIFO) buffer, with constant time insertion and sampling.\n",
    "    '''\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # capacity of the buffer\n",
    "        self.data = []\n",
    "        self.index = 0 # index of the next cell to be filled\n",
    "\n",
    "    def append(self, s, a, r, s_, d):\n",
    "        # TODO\n",
    "        # Don't forget: once the buffer is full, old elements are the first to be popped out.\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # It will be useful to have separate torch.Tensor for the each element type in the sampled minibatch.  \n",
    "        # That is one Tensor for a minibatch of states, another for actions, etc.\n",
    "        batch = random.sample(self.data, batch_size)\n",
    "        return list(map(lambda x:torch.Tensor(x).to(device), list(zip(*batch))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each state, we will need to find the best estimated action, that is $\\arg\\max_a Q(s,a)$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Define a utility function that computes the greedy action from a DQN and a batch of states.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "\n",
    "import torch\n",
    "\n",
    "def greedy_action(network, state):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Fill the blanks in the code below to write a class that keeps a replay buffer as internal attribute and implements the pseudo-code we wrote earlier.  \n",
    "Here are a few tips:\n",
    "- To pick an action at each step and to balance exploration versus exploitation, draw a random value between 0 and 1. If it is below $\\epsilon = 0.2$, pick a random action, otherwise, pick the greedy action in the current state.\n",
    "- A common optimizer (instead of plain SGD) is Adam.\n",
    "- Assume the interaction with the environment will be episodic and run the training for a certain number of episodes.\n",
    "\n",
    "Write your class but don't run this code just yet!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DQN_agent:\n",
    "    def __init__(self, config, model):\n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = ReplayBuffer(config['buffer_size'])\n",
    "        self.epsilon = config['epsilon']\n",
    "        self.model = model.to(device) \n",
    "        self.criterion = # TODO: what is the loss used to train the DQN?\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    def gradient_step(self):\n",
    "        # TODO\n",
    "        # Implement drawing a minibatch and performing one gradient step on the DQN\n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        # TODO\n",
    "        # Implement the algorithm that \n",
    "        #    picks an epsilon-greedy action\n",
    "        #    plays this action\n",
    "        #    adds the sampled transition to the replay buffer\n",
    "        #    takes a gradient step\n",
    "        #    moves on to the next time step\n",
    "        # until max_episodes have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of usage of such a code. It is placed on purpose in a text cell so that you don't run it by mistake.\n",
    "\n",
    "```python\n",
    "breakout = gym.make('BreakoutDeterministic-v4')\n",
    "config = {'gamma': 0.95,\n",
    "          'batch_size': 64,\n",
    "          'nb_actions': breakout.action_space.n,\n",
    "          'buffer_size': 10000000,\n",
    "          'epsilon': 0.2,\n",
    "          'learning_rate': 0.001}\n",
    "BreakoutDQN = BreakoutCNN().to(device)\n",
    "BreakoutLearner = DQN_agent(config, BreakoutDQN)\n",
    "BreakoutLearner.train(breakout, 200)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main reasons for not running this code yet.\n",
    "1. Training a CNN might take a long time. Unless you have a good GPU and a fair amount of time ahead of you (several hours or more), it is recommended to run this computation for a limited number of episodes, on a cloud computing service (or on a dediated machine).\n",
    "2. We have not implemented any monitoring of the training, so if anything fails, we will just have lost time.\n",
    "3. There are many (many!) ways to make this algorithm more efficient.\n",
    "\n",
    "If you really want to try, I recommend training for a single episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you actually run the code above, it is likely that the neural network will not converge to a good estimate of $Q^*$ and might even diverge. There are a couple of extra tricks that improve the previous algorithm, in particular the introduction of *target networks* but we avoid them for the sake of simplicity.\n",
    "\n",
    "Additionally, there are many (really many) ways to make the learning drastically more efficient. The field has been blooming for the last years. Parallelization, sample efficiency, distribution estimation, network stabilization... many hot topics are still open research areas and the progress has been incredibly fast.\n",
    "\n",
    "Below is a video of what you should get (along and) after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"TmPfTpjtdgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec3\"></a> Moving towards real-life problems\n",
    "\n",
    "Ok, that was fun. We now have an intuitive understanding of a learning procedure that moves towards an optimal strategy. Let's take a step back and consider how this can help us for more \"serious\", real-life problems.\n",
    "\n",
    "Consider the following problems:\n",
    "\n",
    "- stabilizing a double inverted pendulum\n",
    "\n",
    "<img src=\"img/double_pendulum.gif\" width=\"200px\">\n",
    "\n",
    "- Scheduling elevator movements\n",
    "\n",
    "<img src=\"img/elevators.jpg\" width=\"200px\">\n",
    "\n",
    "- Fluid flow control\n",
    "\n",
    "<img src=\"img/flow.png\" width=\"200px\">\n",
    "\n",
    "- Game playing\n",
    "\n",
    "<img src=\"img/breakout.jpg\" width=\"200px\">\n",
    "\n",
    "- Traffic jam navigation\n",
    "\n",
    "<img src=\"img/gps.png\" width=\"200px\">\n",
    "\n",
    "But also:\n",
    "- robot motor skill aquisition\n",
    "- agro-ecosystems policy design\n",
    "- supply-chain management and vehicle routing\n",
    "- frequency allocation in LEO telecom satelites\n",
    "- chatbot dialog management\n",
    "- personalized therapeutic strategies\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\"> \n",
    "    \n",
    "**Brainstorming**<br>\n",
    "Can you see why all these problems fall into a common category? Can you describe this category of problems?\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "<ul>\n",
    "    <li> Sequences of decisions (actions), </li>\n",
    "    <li> Stochastic, complex, non-linear dynamics (possibly), </li>\n",
    "    <li> No available formal model, </li>\n",
    "    <li> Goal: closed-loop control policies (strategies, control laws, behaviours), </li>\n",
    "    <li> Goal: find an optimal behaviour. </li>\n",
    "</ul>\n",
    "\n",
    "In technical terms, this is the framework of **Discrete-time Stochastic Optimal Control**. Reinforcement Learning is the set of procedures that try to solve this family of problems **without** the elaboration of a model, using **interaction data** directly.\n",
    "\n",
    "Let us rephrase the above statements with a drawing and a more AI-ish vocabulary.\n",
    "\n",
    "<center><img src=\"img/rl.png\" width=\"500px\"></center>\n",
    "\n",
    "We consider an agent that needs to take a sequence of decisions (*discrete-time*), so that its overall behaviour (*control*) is a good as possible (*optimal*) in an environment whose dynamics are possibly *stochastic*. At each time step, the RL agent observes a *reward signal* indicating how much it gained from the last transition between states. The RL agent tries to find the optimal behaviour without a model of the environment and through interaction data.\n",
    "</details>\n",
    "\n",
    "So the key idea of RL is that powerful enough learning procedures can help tackle these difficult problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Caveat**<br>\n",
    "RL is **not** the solution to all your control problems.\n",
    "<ul>\n",
    "    <li> If you have a reliable model and good analysis tools (Dynamic Programming, Linear Control Theory, Operations Research models...), use them!\n",
    "    <li> Many RL methods are slow, sample-inefficient, unstable, etc. Even if great successes are being obtained, don't be fooled, there is still lots of research to be done before the field reaches a full maturity.\n",
    "    <li> There are probably many other reasons.\n",
    "</ul>\n",
    "I'm a researcher in RL. I hope to convince you that RL is a great investigation topic with an awesome potential. But I also want you to keep a critical eye on the field.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**A few key fundamental issues in RL:**<br>\n",
    "<ul>\n",
    "    <li> The exploration/exploitation trade-off, deceptive rewards,\n",
    "    <li> Value function approximation quality,\n",
    "    <li> Convergence guarantees and speed (sample complexity),\n",
    "    <li> Parametrized policies and value functions, Actor-Critic architectures, etc.\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**A few key practical issues in RL:**<br>\n",
    "<ul>\n",
    "    <li> Reproducibility and statistical representativity,\n",
    "    <li> Robustness and transferability,\n",
    "    <li> Safety,\n",
    "    <li> Computationnal burden, scaling up.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec4\"></a> A bit of theory\n",
    "\n",
    "Let's formalize this a little on the white board. Here are the topics we should cover to provide a sound foundation to the experiment above:\n",
    "- Markov Decision Processes,\n",
    "- Policies, optimality criteria and Value functions,\n",
    "- Bellman's optimality principle (and the corresponding equations),\n",
    "- Q-learning is a stochastic optimization procedure.\n",
    "\n",
    "All these topics are covered in (for instance) the \"[RL fundamentals](https://rl-vs.github.io/rlvs2021/rl-fundamentals.html)\" class of the 2021 Reinforcement Learning Virtual School, which you can freely access.\n",
    "\n",
    "We can also decide to skip this part if you want to play a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec5\"></a> Your turn to play\n",
    "\n",
    "There are endless practice opportunities. I suggest the following exercises. They are meant to help you practice, but also to trigger questions and discussions in class.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Adapt the previous code to train a good control policy on CartPole.\n",
    "- Use a simple network with 2 hidden layers and 24 ReLU neurons on each layer.\n",
    "- To encourage early exploration and let the agent exploit at later stages, take a constant $\\epsilon_{max}$ value during $\\tau_{delay}$ time steps, then substract $\\epsilon_{step}$ from $\\epsilon$ at every time step until you reach time $\\tau_{period}$. Take $\\epsilon_{max}=1$, $\\epsilon_{min=0.01}$, $\\tau_{delay}=20$ and $\\tau_{period}=1000$.  \n",
    "- After each training episode store the episode's cumulated return for monitoring.\n",
    "- Train for 200 episodes, with a learning rate of $0.001$, a batch size of $20$, $\\gamma=0.95$ and a replay buffer of maximum $1000000$ samples.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex5.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to work reasonably well, although not perfectly. Let's take a step back and answer a few (open) questions.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "- Is the episodic return plot (above) actually a good indicator of the greedy policy's efficiency ? Why or why not?\n",
    "- Is the linear decrease of $\\epsilon$ a good schedule?\n",
    "- Should we change the optimizer?\n",
    "- Is it a good (or a bad) idea to take a gradient step for each acquired sample?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "\n",
    "- The episodic return indicates how the $\\epsilon$-greedy policy performs. So it includes the influence of the random exploration. Even though $\\epsilon$ becomes small at the end of the learning, in some more difficult environments, the actual value of the greedy policy might be different from that of the $\\epsilon$-greedy one.\n",
    "- The linear decrease of $\\epsilon$ defines a rather inefficient exploration scheme. Some state-action pairs that are close to the starting state are tried very often, are over-represented in the replay buffer, and could probably avoid exploration altogether. Conversely, some rarely visited states might lack samples. So one could wish for a more *contextual* notion of exploration; one that would depend on the current state for instance, or one that would actually *plan* the exploration for better *sample efficiency*.\n",
    "- It is interesting and easy to change the optimizer. For CartPole, it seems that ADAM is a better choice than RMSprop but this is not a general rule.\n",
    "- Why take a single gradient step on the network's loss when a new sample is collected? Taking several gradient steps can greatly accelerate the convergence in terms of number of collected samples. There is a limitation to this idea though: if the policy changes too fast, then the samples present in the replay buffer might now remain representative of the states and actions this policy will visit, and this can slow down (in the best case) convergence. So more gradient steps at every sample seem like a good idea but it is actually quite a dangerous practice that requires caution.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "In the theory part of the class, we have introduced Approximate Value Iteration. Why is DQN an approximate value iteration algorithm? \n",
    "By noting that the training loss is a (reasonably good) proxy for the network's approximation error, what could we do to improve this approximation error and thus the convergence of DQN?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Ready to see the answer? (click to expand)</b></summary>\n",
    "We have seen that AVI corresponds to the sequence $Q_{n+1} = \\mathcal{A} T^* Q_n$, with $\\mathcal{A}$ an approximation operator and $T^*$ the Bellman optimality operator.\n",
    "\n",
    "DQN actually defines a sequence of functions where one obtains $Q_{n+1}$ by taking a single stochastic gradient step from $Q_n$. Specifically, one samples a minibatch of $x=(s,a)$, draws realizations of the corresponding $T^* Q_n (s,a)$ under the form $y=r+\\gamma\\max_{a'}Q_n(s',a')$, and then takes a gradient step to bring $Q_n$ closer to these realizations. This defines the approximation operator $\\mathcal{A}$. \n",
    "\n",
    "We remark that the training loss is a proxy for the approximation error. And since the approximation error conditions the improvement in AVI, we would like to keep it small. One way to do this could be to take *several gradient steps* with respect to the same $Q_n$ before defining $Q_{n+1}$. \n",
    "\n",
    "In practice, this boils down to defining two separate networks: one for $Q_n$, that is kept fixed for a number $C$ of gradient steps, and one for $Q_{n+1}$ that is learned by taking these gradient steps with respect to $Q_n$. The network for $Q_n$ is called a **target network** and is an essential ingredient for efficient DQN algorithms.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common pratice to stabilize learning is to clip the value of the loss' gradient between $-1$ and $1$. This is not such an uncommon trick, it actually amounts to using an L2 loss for values of the loss between $-1$ and $1$ and an L1 loss outside of this domain. This is also know as the [Huber Loss](https://en.wikipedia.org/wiki/Huber_loss) or the [smooth L1 loss](https://pytorch.org/docs/stable/nn.html#smoothl1loss).\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Modify your DQN agent class from the previous exercises to include a target network (with $C$ in the order of $100$), a fixed number of gradient steps per acquired sample, and the clipping of the error term. You can also use Adam for improved efficiency.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex6.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Run your code to learn an efficient policy for CartPole. Don't forget to reset the network before you start learning.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/ex7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercise:**  \n",
    "Train on SwingUp.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE YOUR CODE HERE\n",
    "# If you get stuck, uncomment the line in the next cell to load a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/ex8.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did your DQN learn to avoid the -10 penalty instead of swinging up? The exploration question becomes a crucial issue, doesn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"sec5\"></a> Wrapping up and going further\n",
    "\n",
    "We have seen that Reinforcement Learning is the set of procedures that aim at solving the family of Discrete-time Stochastic Optimal Control problems, without the elaboration of a model, using interaction data directly.\n",
    "\n",
    "It builds upon the generic framework of Markov Decision Processes.\n",
    "\n",
    "We have introduced (without technicalities) the key ideas that lead to the DQN family of algorithms, and then formalized it to the more general set of algorithms that derive from Approximate Dynamic Programming (which includes approximate value iteration). On the way, we showed that DQN was an specific case of an approximate value iteration algorithm.\n",
    "\n",
    "Departing from this, we can explore further:\n",
    "- how can one define better exploration schemes?\n",
    "- what happens when the action set is not discrete and becomes continuous?\n",
    "- are neural networks the only possible function approximators and what makes a value function learning procedure efficient?\n",
    "\n",
    "There are other families of Reinforcement Learning algorithms that do not belong to approximate dynamic programming family. They still aim at solving the same problem, but rely on other fundamental properties. Among these, we should at least mention:\n",
    "- policy gradient methods and the rich family of actor-critic methods,\n",
    "- evolutionary reinforcement learning methods.\n",
    "\n",
    "These keywords define the global current literature on Reinforcement Learning. Many resources (like the [RLVS online classes](https://rl-vs.github.io/rlvs2021/)) tackle these questions (most of them are still active research fields)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "300px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
