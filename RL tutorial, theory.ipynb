{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "794dc34c-5d58-406a-b219-bfd33811e4e3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en) | <a href=\"https://github.com/erachelson/RLtuto\">https://github.com/erachelson/RLtuto</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c503ab3-66a8-4691-bc58-6990e9f1c39d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">A walk in the garden of foundational ingredients, <br> theorems and algorithms in reinforcement learning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef116ef6-623e-41b9-be85-8d1d9fff98df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Welcome to the RL restaurant! On the menu:\n",
    "- Central intuitions for RL (section 1)\n",
    "- Setting the stage (sections 2, 3, 4)\n",
    "- Finally some learning (sections 5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df590bd3-346c-4f45-ad8d-62f77d4645d6",
   "metadata": {},
   "source": [
    "My intention in this tutorial:\n",
    "- a compendium of definitions, theorems and algorithms that you can re-use,\n",
    "- minimal text, lots of room for discussion,\n",
    "- rigorous mathematical understanding and algorithmic notions at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fef8e-7a74-4762-88cf-5d7949ffce24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Intuitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969eecf2-c3b0-4f7e-888c-d476307dc273",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## A medical prescription example\n",
    "\n",
    "<center><img src=\"img/patient-doctor.png\" style=\"height: 200px;\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019eca7-7119-4e3d-9f70-7aefb06cb5ac",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Patient variables\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_file.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Patient state now: $S_0$  <br>\n",
    "Future states: $S_t$\n",
    "</center>\n",
    "\n",
    "$S_t$ random variable  \n",
    "$\\mathcal{S}$ patient description space, state space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc77f0-c701-4bd7-8ac0-35630e086426",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Prescription\n",
    "\n",
    "<center>\n",
    "<img src=\"img/prescription.png\" style=\"height: 100px;\"> </img> <br>\n",
    "Prescription: $\\left( A_t \\right)_{t\\in\\mathbb{N}} = (A_0, A_1, A_2, ...)$\n",
    "</center>\n",
    "\n",
    "$A_t$ random variable  \n",
    "$\\mathcal{A}$ prescription space, action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37eedd7-2a5a-4a3b-a414-aa071eecdd36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Patient evolution\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"img/patient_evolution.png\" style=\"height: 100px;\"> </img> <br>\n",
    "    $\\mathbb{P}(S_t)$?\n",
    "</center>\n",
    "\n",
    "$\\left( S_t \\right)_{t\\in\\mathbb{N}}$ random process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3c717-3f64-4ea8-bbcc-d8c7f5611b56",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Physician's goal\n",
    "\n",
    "<center><img src=\"img/patient_happy.png\" style=\"height: 100px;\"> </img> <br></center>\n",
    "\n",
    "$$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)?$$\n",
    "\n",
    "Make (keep) patient healthy, from $S_0$, until time horizon.  \n",
    "$J \\left( \\left(S_t\\right)_{t\\in \\mathbb{N}}, \\left( A_t \\right)_{t\\in \\mathbb{N}} \\right)$: how good is a trajectory in the joint $S\\times A$ space?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c3aeef-d7a6-48a4-ae1c-f4b903086c5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Wrap-up\n",
    "\n",
    "- Patient state $S_t$, random variable with values in $\\mathcal{S}$,\n",
    "- Physician instruction $A_t$, random variable with values in $\\mathcal{A}$,\n",
    "- Prescription $\\left( A_t \\right)_{t\\in\\mathbb{N}}$, sequence of random variables, random process  \n",
    "- Patient's evolution $\\mathbb{P}(S_t)$,  \n",
    "- Patient's state trajectory $\\left( S_t \\right)_{t\\in\\mathbb{N}}$, random process, \n",
    "- Patient's full trajectory $\\left( S_t, A_t \\right)_{t\\in\\mathbb{N}}$, random process, \n",
    "- Value of a trajectory $J \\left( \\left(S_t, A_t \\right)_{t\\in \\mathbb{N}} \\right)$.  \n",
    "\n",
    "It seems reasonable that the physician's recommendation $\\mathbb{P}(A_t)$ at step $t$ be dependent on previously observed states $\\left(S_0, \\ldots, S_t\\right)$ and recommended treatments $\\left(A_0, \\ldots, A_{t-1}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f888b-406f-48fc-a5c7-829a88674515",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Common misconception\n",
    "\n",
    "You will often see the following type of drawing, along with a sentence like \"RL is concerned with the problem on an agent performing actions to control an environment\". \n",
    "\n",
    "<center><img src=\"img/misconception.png\" style=\"height: 300px;\"></img></center>\n",
    "\n",
    "Not false, but prone to (anthropomorphic) misconceptions:  \n",
    "No separation between a *state of the agent* and a *state of the environment*.\n",
    "\n",
    "Less shiny but more accurate drawing:  \n",
    "\n",
    "<center><img src=\"img/dynamic.png\" style=\"height: 300px;\"></img></center>\n",
    "\n",
    "A *System to control*, described through its observed *state*, controlled by the application of actions issued from a *policy* or *control law*. The process of *learning* this policy is what RL is concerned with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71e0e5-8757-41ff-865e-96373f71abc4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ingredients (definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0e7fc-541d-4dfc-a93e-118945857ac3",
   "metadata": {},
   "source": [
    "## Markov decision processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ee727-e27a-4987-b47a-5cf145e491cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Definition: Markov Decision Process (MDP)</b><br>\n",
    "A Markov Decision Process is given by:\n",
    "<ul>\n",
    "<li> A measurable set of states $\\mathcal{S}$\n",
    "<li> A measurable set of actions $\\mathcal{A}$\n",
    "<li> A (Markovian) transition model $\\mathbb{P}\\left(S_{t+1} | S_t, A_t \\right)$, noted $p(s'|s,a)$\n",
    "<li> A reward model $\\mathbb{P}\\left( R_t | S_t, A_t, S_{t+1} \\right)$, noted $r(s,a)$ or $r(s,a,s')$\n",
    "<li> A set of discrete decision epochs $\\mathcal{T}=\\{0,1,\\ldots,h\\}$\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fb545-8022-41b8-8ef9-c143d1394aab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Examples**  \n",
    "- video games,\n",
    "- robots,\n",
    "- agro-ecosystems,\n",
    "- biological systems,\n",
    "- financial marketplaces,\n",
    "- communication networks,\n",
    "- supply-chain systems, etc.\n",
    "\n",
    "In short: any fully-observable \"$s_{t+1} = f(s_t,a_t) + noise$\" dynamical system.\n",
    "\n",
    "Today's tutorial: $h=+\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca14256-dfe5-41cf-a75e-bce97f69aa03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The \"frozen lake\" toy MDP\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f478e-cbd3-4aa1-b68c-496aaa953070",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "[Link to the Gymnasium website](https://gymnasium.farama.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9175c06-080c-4f1a-b24d-4adae60c72c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# State and action space\n",
    "print(\"State space: \", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de07d1e-b807-426f-87d7-6a169c023b1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "actions = {fl.LEFT: '\\u2190', fl.DOWN: '\\u2193', fl.RIGHT: '\\u2192', fl.UP: '\\u2191'}\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fc19f2-a2a4-4f4f-9f25-93aec538014e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transition model\n",
    "s=2\n",
    "a=1\n",
    "print(\"Example of transition: (s=\", s, \n",
    "      \",a=\", actions[a], \n",
    "      \") \\u27FF s=\", env.unwrapped.P[s][a][0][1], \n",
    "      \" with proba \", env.unwrapped.P[s][a][0][0], \n",
    "      \".\", sep='')\n",
    "print(\"All transitions from (s=\", s, \n",
    "      \",a=\", actions[a], \n",
    "      \"):\", sep='')\n",
    "print(env.unwrapped.P[s][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebce60e-6251-4107-a6b8-7d0bb2dd730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "s, prob = env.reset()\n",
    "print(\"Initial state:\", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a9933-4b06-4f2b-9ebe-7bf78550a7a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take an action in the MDP\n",
    "a=2\n",
    "print(\"Old state:\", s)\n",
    "s, r, done, truncated, info = env.step(a)\n",
    "print(\"Action:   \", a, \" (\", actions[a], \")\", sep=\"\")\n",
    "print(\"New state:\", s)\n",
    "print(\"Reward:   \", r)\n",
    "print(\"Game over:\", done)\n",
    "print(\"Truncated:\", truncated)\n",
    "plt.imshow(env.render());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a71939-666f-46d6-ade1-bda2509d52ec",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a418e-de3b-4ed6-8886-a5c19cff9e78",
   "metadata": {},
   "source": [
    "Formally, how does one write the behavior of an agent?\n",
    "This behavior specifies how to choose actions at each time step:\n",
    "$$A_t \\sim \\pi_t.$$\n",
    "\n",
    "Let $\\Delta_\\mathcal{A}$ be the set of probability measures on the action space $\\mathcal{A}$. Then the law $\\pi_t$ of $A_t$ belongs to $\\Delta_\\mathcal{A}$.  \n",
    "$\\pi_t$ is called the **decision rule** at step $t$, it is a distribution over the action space $\\mathcal{A}$.  \n",
    "The collection $\\pi = \\left(\\pi_t \\right)_{t\\in \\mathcal{T}}$ is called a **policy**.\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Definition: policy $\\pi$</b><br>\n",
    "A policy $\\pi$ is a sequence of decision rules $\\pi_t$: $\\pi = \\{\\pi_t\\}_{t\\in \\mathcal{T}}$, with $\\pi_t \\in \\Delta_\\mathcal{A}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1659f-b497-45e9-8fe9-3650c8b8493f",
   "metadata": {},
   "source": [
    "Remark: $\\pi_t$ might depend on the full history of $S_0,A_0,\\ldots,S_{t-1},A_{t-1},S_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa4ea1-6b24-436b-8a89-37c0142c3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deterministic, stationary, state-dependent policy which always moves right\n",
    "import numpy as np\n",
    "pi = np.ones(env.observation_space.n)\n",
    "print(\"A deterministic, stationary, state-dependent policy:\", pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eda119-1843-4134-9019-ed639b4ec696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A randomly-drawn deterministic, stationary, state-dependent policy\n",
    "max_action = env.action_space.n - 1\n",
    "pi = np.round(max_action * np.random.random((env.observation_space.n)))\n",
    "print(\"A randomly-drawn deterministic, stationary, state-dependent policy:\", pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b00a18-82df-4ea8-bcf4-4e09f2c3225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi):\n",
    "    for row in range(env.unwrapped.nrow):\n",
    "        for col in range(env.unwrapped.ncol):\n",
    "            s = row*env.unwrapped.ncol+col\n",
    "            print(actions[pi[s]], end='')\n",
    "        print()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e6b62c-a88d-4a1b-8498-9bdc07aa61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26963f-cb8e-4b92-8fdd-8d80c77bf9cf",
   "metadata": {},
   "source": [
    "## Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b6f5c-0e75-4670-b80d-56ea93997d14",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Definition: the **state return** random variable:\n",
    "$$G^\\pi(s) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s,\\\\ A_t \\sim \\pi_t,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69a1c4-8827-49e6-a94b-c5daa54421a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Definition: value function $v^\\pi$ of a policy $\\pi$ under a $\\gamma$-discounted criterion</b><br>\n",
    "$$v^\\pi : \\left\\{\\begin{array}{ccl}\n",
    "\\mathcal{S} & \\rightarrow & \\mathbb{R}\\\\\n",
    "s & \\mapsto & v^\\pi(s)=\\mathbb{E}\\left( \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\bigg| S_0 = s, \\pi \\right)\\end{array}\\right. $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35d19a5-8dc7-471f-b782-03f7073b8d2c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Definition: the **state-action return** random variable:\n",
    "$$G^\\pi(s,a) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi_t\\textrm{ for }t\\geq 1,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\"><b>Definition: state-action value function $q^\\pi$</b><br>\n",
    "$$q^\\pi(s,a) = \\mathbb{E}\\left( \\sum\\limits_{t=0}^\\infty \\gamma^t r\\left(S_t, A_t, S_{t+1}\\right) \\bigg| S_0 = s, A_0=a, \\pi \\right)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e754d-782b-4e12-b14d-ad2d0f5ddebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo evaluations\n",
    "import numpy as np\n",
    "pi0 = 0*np.ones(env.observation_space.n)\n",
    "pi1 = 1*np.ones(env.observation_space.n)\n",
    "pi2 = 2*np.ones(env.observation_space.n)\n",
    "pi3 = 3*np.ones(env.observation_space.n)\n",
    "pi4 = np.round(3*np.random.random((env.observation_space.n)))\n",
    "pi5 = np.array([2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 1, 0, 2, 2, 2, 2])\n",
    "policies = [pi0, pi1, pi2, pi3, pi4, pi5]\n",
    "rollouts = 100\n",
    "horizon = 200\n",
    "gamma = .99\n",
    "v0 = 0\n",
    "\n",
    "for pi in policies:\n",
    "    for ep in range(rollouts):\n",
    "        s, _ = env.reset()\n",
    "        G = 0\n",
    "        for t in range(horizon):\n",
    "            a = pi[s]\n",
    "            s, r, done, _, _ = env.step(a)\n",
    "            G += gamma**t * r\n",
    "            if done:\n",
    "                break\n",
    "        v0 += G\n",
    "    v0 = v0/rollouts\n",
    "    print_policy(pi)\n",
    "    print(\"    v^pi(s_0) =\", v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbb2bd-bac0-493f-b4f4-420742585326",
   "metadata": {},
   "source": [
    "## Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6dba5-e302-49ca-bc0f-49c9e576808f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Definition: optimal policy $\\pi^*$**  \n",
    "$\\pi^*$ is said to be optimal iff $\\pi^* \\in \\arg\\max\\limits_{\\pi} v^\\pi$.  \n",
    "    \n",
    "A policy is optimal if it **dominates** over any other policy in every state:\n",
    "$$\\pi^* \\textrm{ is optimal}\\Leftrightarrow \\forall s\\in \\mathcal{S}, \\ \\forall \\pi, \\ v^{\\pi^*}(s) \\geq v^\\pi(s)$$\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Definition: optimal value function**  \n",
    "$v^*$ is the unique value function of any optimal policy.  \n",
    "$q^*$ is the unique state-action value function of any optimal policy.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c441ac1-c246-467e-95de-8bb28945f242",
   "metadata": {},
   "source": [
    "# Optimal policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f20c6fd-c311-48c3-b628-7aac29ae580c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><b>Theorem: family of optimal policies</b><br>\n",
    "There always exists at least one optimal stationary, deterministic, memoryless policy.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd743b92-9b24-41d1-aac3-96a4ce3b9490",
   "metadata": {},
   "source": [
    "Consequence: much easier to search for than non-stationary, stochastic, history-dependent policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872e89d2-4621-4647-9f14-14a93063164a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Theorem: the policy optimization problem:**  \n",
    "Provided $\\rho_0$ has non-zero probability mass on all states, an optimal policy is a solution to $\\max_\\pi J(\\pi) = \\mathbb{E}_{s_0\\sim \\rho_0}[v^\\pi(s_0)]$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c863cc3a-fbfc-4c2e-985d-44406181ea66",
   "metadata": {},
   "source": [
    "Consequence: single scalar optimization problem, rather than $|\\mathcal{S}|$ coupled optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c60fab-80b6-46ed-b026-2c35ca16b85c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Definition: greediness operator**  \n",
    "For deterministic policies:\n",
    "$$\\pi \\in \\mathcal{G} q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{a\\in \\mathcal{A}} q(s,a)$$\n",
    "\n",
    "This can be extended to stochastic policies:\n",
    "$$\\pi \\in \\mathcal{G} q, \\Leftrightarrow \\pi(s) \\in \\arg\\max_{\\delta \\in \\Delta_\\mathcal{A}} \\mathbb{E}_{a\\sim\\delta} \\left[q(s,a)\\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcd7a0-e0be-426c-a9b5-2eecaf5373f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Theorem: Optimal greedy policy**  \n",
    "$\\pi \\in \\mathcal{G}q^*$ is an optimal (deterministic or stochastic) policy.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abc48c-c549-4eff-94b5-2f1a3736a255",
   "metadata": {},
   "source": [
    "Consequence: finding $q^*$ provides $\\pi^*$ if one can solve this $\\max_a$ easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76e5e4-b01c-4b78-a6c0-eeba90d4ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greedy policy from a random q-function\n",
    "import numpy as np\n",
    "q = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "def greedyQpolicy(Q):\n",
    "    pi = np.zeros((env.observation_space.n),dtype=int)\n",
    "    for s in range(env.observation_space.n):\n",
    "        pi[s] = np.argmax(Q[s,:])\n",
    "    return pi\n",
    "\n",
    "print(\"q-function:\\n\", q)\n",
    "print(\"greedy deterministic policy:\\n\",greedyQpolicy(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66b6a9-d7d6-4bb0-a8a2-1284c348f357",
   "metadata": {},
   "source": [
    "# Characterizing value functions: the Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79f3711-b4b4-4840-ae07-3e2490bba7a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Theorem: the evaluation equation**  \n",
    "$v^\\pi$ obeys the (full rank) linear system of equations:\n",
    "$$v^\\pi\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ v^\\pi(s') \\right], \\forall s\\in \\mathcal{S}$$\n",
    "\n",
    "$q^\\pi$ obeys the (full rank) linear system of equations:\n",
    "$$q^\\pi\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ q^\\pi(s',\\pi(s')) \\right], \\forall s, a\\in \\mathcal{S}\\times\\mathcal{A}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a64cf5-f527-4d1e-bd83-9e89103d266c",
   "metadata": {},
   "source": [
    "Comment: this is a dynamic programming equation. The value $v^\\pi(s)$ of the trajectory starting in $s$ is the sum of the first step's reward $r(s,\\pi(s))$ and the discounted value from the outcome state $v^\\pi(s')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66b28-62bc-43a5-b23e-09c3054f12fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Definition: Bellman evaluation operator $T^\\pi$**\n",
    "$$(T^\\pi v)\\left(s\\right) = r(s,\\pi(s)) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,\\pi(s))} \\left[ v(s') \\right]$$\n",
    "$$(T^\\pi q)\\left(s,a\\right) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ q(s',\\pi(s')) \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d653b2-792f-4823-8e82-dc845c3d7dab",
   "metadata": {},
   "source": [
    "The evaluation equation:  \n",
    "$v^\\pi$ is the unique solution to $v=T^\\pi v$,  \n",
    "$q^\\pi$ is the unique solution to $q = T^\\pi q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e0c44-1a93-4f77-9571-8692400fd65c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Theorem: the optimality equation**  \n",
    "$$v^*(s) = \\max\\limits_{a\\in \\mathcal{A}} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} v^*(s') \\right]$$\n",
    "$$q^*(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in \\mathcal{A}} q^*(s',a') \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82404523-400c-404c-9fcf-b47a97a4c5c0",
   "metadata": {},
   "source": [
    "Comment: this is a dynamic programming equation again. The largest expected return $v^*(s)$ is the largest possible sum of the one-step reward $r(s,a)$ and the optimal value from to outcome state $v^*(s')$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30556ebe-5a6b-40ba-a91a-20c007fb0adf",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Definition: Bellman optimality operator**  \n",
    "$$(T^*v)(s) = \\max\\limits_{a\\in \\mathcal{A}} \\left[ r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} v(s') \\right]$$\n",
    "$$(T^*q)(s,a) = r(s,a) + \\gamma \\mathbb{E}_{s'\\sim p(s'|s,a)} \\left[ \\max_{a'\\in \\mathcal{A}} q(s',a') \\right]$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8181cb-1ef1-4763-964c-80abb1e9fab3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Theorem: $T^*$ and $T^\\pi$ are $L_\\infty$-contraction mappings**  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b12f05-9412-46dc-9210-95eac6213360",
   "metadata": {},
   "source": [
    "Consequence: unicity of a solution to the optimality equation.  \n",
    "$v^*$ is the unique solution to $v=T^*v$,  \n",
    "$q^*$ is the unique solution to $q=T^*q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c81691-001d-40b2-9f36-e14590c29a4c",
   "metadata": {},
   "source": [
    "# Dynamic programming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448a2ab5-a4bc-467f-a384-5e3e44ba1d6a",
   "metadata": {},
   "source": [
    "$v^\\pi$ is the limit of the sequence $v_{n+1} = T^\\pi v_n$, $q^\\pi$ is the limit of the sequence $q_{n+1} = T^\\pi q_n$,  \n",
    "$v^*$ is the limit of the sequence $v_{n+1} = T^* v_n$, $q^*$ is the limit of the sequence $q_{n+1} = T^* q_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cd320-1a19-48a2-939e-8900264de719",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Algorithm: value iteration**  \n",
    "The algorithm that computes the sequence $Q_{n+1} = T^* Q_n$ for a finite number of iterations is called **value iteration**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84377e7-ef35-4cd8-b288-2aea984224f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value iteration\n",
    "# Input: nb iterations, q0\n",
    "# Output: q*\n",
    "\n",
    "import gymnasium as gym\n",
    "import gymnasium.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "# use render_mode=\"human\" to open the game window\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "nb_iter = 20\n",
    "gamma = 0.9\n",
    "q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "qopt_sequence = [q]\n",
    "for i in range(nb_iter):\n",
    "    qnew = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for x in range(env.observation_space.n):\n",
    "        for a in range(env.action_space.n):\n",
    "            outcomes = env.unwrapped.P[x][a]\n",
    "            for o in outcomes:\n",
    "                p = o[0]\n",
    "                y = o[1]\n",
    "                r = o[2]\n",
    "                qnew[x,a] += p * (r + gamma*np.max(q[y,:]) )\n",
    "    q = qnew\n",
    "    qopt_sequence.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d4689-225a-4a49-965a-87efa38518f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_policy(greedyQpolicy(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0101f-444e-47ed-8fea-162cc60f22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the convergence of the sequence\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "residuals = []\n",
    "for i in range(1, len(qopt_sequence)):\n",
    "    residuals.append(np.max(np.abs(qopt_sequence[i]-qopt_sequence[i-1])))\n",
    "\n",
    "plt.plot(residuals)\n",
    "plt.figure()\n",
    "plt.semilogy(residuals);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd99491-8f34-4504-9361-27395ab9f37d",
   "metadata": {},
   "source": [
    "Remark:  \n",
    "$T^*q = T^\\pi q$ with $\\pi \\in \\mathcal{G} q$.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Algorithm: value iteration (reformulated)**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n, \\ q_{n+1} = T^{\\pi_n} q_n$$\n",
    "</div>\n",
    "\n",
    "VI converges to $q^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f346578-9808-4f26-bdeb-f232db160393",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Algorithm: policy iteration**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n, \\ q_{n+1} \\textrm{ is a solution to } q=T^{\\pi_n} q$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c195c03-d0a4-4297-8869-88adb5b12ba4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Algorithm: modified policy iteration**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n, \\ q_{n+1} = {(T^{\\pi_n})}^m q_n$$\n",
    "</div>\n",
    "\n",
    "VI is MPI with $m=1$.  \n",
    "PI is MPI with $m=\\infty$.\n",
    "\n",
    "All converge to $q^*$ and $\\pi^*$ (whatever their initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937d8a32-211e-48a2-8cdc-bc8fe3679d4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Theorem: time complexity of modified policy iteration**  \n",
    "\n",
    "TODO\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa569015-b8ed-495e-a836-3e28133669dd",
   "metadata": {},
   "source": [
    "# Approximate dynamic programming algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc938ac5-4b60-4a32-805f-a2b2042b0e86",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Approximate value iteration** is the algorithm that computes the sequence $Q_{n+1} = \\mathbb{A} T^* Q_n$, where $\\mathbb{A}$ is an approximation procedure.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c3844-bb20-4986-83e9-bf5cecb58b21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Approximate value iteration (reformulated)**\n",
    "$$\\pi_n \\in \\mathcal{G} q_n, \\ q_{n+1} = \\mathbb{A} T^{\\pi_n} q_n$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c40e86-11c1-4eaa-a235-e63f1648156a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "**Theorem: asymptotical behavior of AVI**  \n",
    "\n",
    "If $\\| f-\\mathbb{A}f \\|_\\infty \\leq \\epsilon, \\forall f \\in \\mathbb{R}^{SA}$,\n",
    "\n",
    "$$\\exists N\\in \\mathbb{N}, \\textrm{such that }\\| Q^* - Q_n \\|_\\infty \\leq \\frac{\\epsilon}{1-\\gamma}, \\forall n\\leq N.$$\n",
    "\n",
    "Let $\\pi_n$ be the greedy policy with respect to $Q_n$,\n",
    "$$\\|Q^*-Q^{\\pi_n}\\|_\\infty \\leq \\frac{2\\gamma\\epsilon}{(1-\\gamma)^2}, \\forall n \\leq N.$$\n",
    "\n",
    "</div>\n",
    "Warning: AVI does not (necessarily) converge!  \n",
    "But it reaches policies whose values are close to optimal.\n",
    "\n",
    "Info: similar bounds in weighted L2 norm (more suited to learning).\n",
    "\n",
    "TODO: add bounds in L2 norm with concentration inequalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea37531c-6d8c-4ab2-ac95-cafa6b093165",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Approximate modified policy iteration**\n",
    "$$\\pi_n \\in \\mathcal{G} q_n, \\ q_{n+1} = \\mathbb{A} {(T^{\\pi_n})}^m q_n$$\n",
    "</div>\n",
    "\n",
    "TODO: concentration inequalities for AMPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759d284-1bfc-4cb7-b552-279f4932e112",
   "metadata": {},
   "source": [
    "Example of $\\mathbb{A}$: a procedure that minimizes $L(\\theta) = \\mathbb{E}_{x,y} [ (f_\\theta(x) - y)^2 ]$, or more generally $L(\\theta) = \\mathbb{E}_{x,y} [ \\ell(f_\\theta(x), y) ]$.\n",
    "\n",
    "Consequence: AVI (or AMPI) can be cast as a sequence of supervised learning problems if we can have samples of $(T{^{\\pi})}^m q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd877ae0-aa1e-4da7-b7d2-26201b8605b5",
   "metadata": {},
   "source": [
    "# Learning for policy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff47267-828f-4fcf-be88-328c27f1eecd",
   "metadata": {},
   "source": [
    "The **return** random variable:\n",
    "$$G^\\pi(s,a) = \\sum\\limits_{t = 0}^\\infty \\gamma^t R_t \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi_t\\textrm{ for }t\\geq 1,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "$$q^\\pi(s,a) = \\mathbb{E}[G^\\pi(s,a)]$$\n",
    "\n",
    "$q^\\pi(s,a)$ is a minimizer of $L(q) = \\mathbb{E} \\left[ \\left( q - G^\\pi(s,a) \\right)^2 \\right]$.   \n",
    "Getting samples from $G^\\pi(s,a)$ = Monte Carlo rollouts.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Policy evaluation as stochastic approximation**  \n",
    "If we can obtain independent realizations $g^\\pi(s,a)$ of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic approximation updates of $q$ under the form:\n",
    "$$q(s,a) \\leftarrow q(s,a) + \\alpha \\left(g^\\pi(s,a) - q(s,a)\\right).$$\n",
    "Then $q$ converges to $q^\\pi$.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Policy evaluation as stochastic gradient descent (1/2)**  \n",
    "If we can access a set $\\left\\{g^\\pi_i(s,a)\\right\\}_{i\\in [1,N]}$ of $N$ independently drawn realizations of $G^\\pi(s,a)$ in all $s,a$, we can perform stochastic gradient descent updates of $q$ in each $s,a$ independently, under the form:\n",
    "$$q(s,a) \\leftarrow q(s,a) + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[g^\\pi_i(s,a) - q(s,a)\\right].$$\n",
    "Then $q$ converges to $q^\\pi$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd5701-d4f7-4975-a4b0-b20d2360f4ee",
   "metadata": {},
   "source": [
    "The formulation above requires updates in each $(s,a)$ separately.  \n",
    "Risk minimization problem with parametric $q_\\theta$:\n",
    "$$L(\\theta) = \\mathbb{E}_{\\substack{(s,a)\\sim \\rho\\\\ g^\\pi(s,a)\\sim G^\\pi(s,a)}} \\left[ \\left( q_\\theta(s,a) - g^\\pi(s,a) \\right)^2\\right].$$\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Policy evaluation as stochastic gradient descent (2/2)**  \n",
    "Provided we have a set $\\{(s_i,a_i,g^\\pi_i\\}_{i\\in [1,N]}$ where the $(s_i,a_i)$ are independent realizations of $(s,a)$ drawn according to $\\rho$ and $g^\\pi_i$ are independent realizations of $G^\\pi(s_i,a_i)$, then we can perform stochastic gradient descent updates of the parameters $\\theta$ of a parametric function $q_\\theta$, under the form:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\frac{1}{N} \\sum_{i=1}^N \\left[g^\\pi_i - q_\\theta(s_i,a_i) \\right] \\nabla_\\theta q_\\theta(s_i,a_i).$$\n",
    "Then $q_\\theta$ converges to some approximation of $q^\\pi$ on the support of $\\rho$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c852-284b-4fc1-a6ad-f57376be879f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Definition: the **$m$-step bootstrapped return** random variable:\n",
    "$$G^\\pi_m(s,a,q) = \\sum\\limits_{t = 0}^{m-1} \\gamma^t R_t + \\gamma^m q(S_m, A_m) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_t \\sim \\pi(S_t)\\textrm{ for }t>0,\\\\ S_{t+1}\\sim p(\\cdot|S_t,A_t),\\\\R_t = r(S_t,A_t,S_{t+1}).\\end{array}$$\n",
    "</div>\n",
    "$${(T^\\pi)}^m q = \\mathbb{E}[G^\\pi_m(s,a,q)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49306c-9612-4e26-9d25-0f3886921d0b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Definition: the **(1-step) bootstrapped return** random variable:\n",
    "$$G^\\pi_1(s,a,q) = R_0 + \\gamma q(S_1, A_1) \\quad \\Bigg| \\quad \\begin{array}{l}S_0 = s, A_0=a\\\\ A_1 \\sim \\pi(S_1),\\\\ S_{1}\\sim p(\\cdot|S_0,A_0),\\\\ R_0 = r(S_0,A_0,S_{1}).\\end{array}$$\n",
    "</div>\n",
    "$${T^\\pi} q = \\mathbb{E}[G^\\pi_1(s,a,q)]$$\n",
    "\n",
    "Consequence: getting samples from $G^\\pi_1(s,a,q)$ enables learning ${T^\\pi} q$.  \n",
    "Getting samples from $G^\\pi_1(s,a,q)$ = getting samples $r\\sim R_0$, $s'\\sim p(\\cdot|s,a)$ and $a'\\sim \\pi(s')$, then summing $r + \\gamma q(s',a')$.  \n",
    "Sampling the bootstrapped return: dynamic programming as a sequence of supervised learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38441c-a0e6-4296-a0ff-cd1d854d4e20",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Approximate dynamic programming as a sequence of risk minimization problems.**  \n",
    "Approximate dynamic programming can be cast as finding the sequence of functions $q(s,a;\\theta_n)$ defined by $\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta)$, with\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( q(s,a;\\theta) - G^\\pi_1(s,a,q_n) \\right)^2 \\right].$$\n",
    "\n",
    "If this risk is differentiable, provided one can draw a mini-batch of independently and identically drawn samples $\\left\\{\\left(s_i,a_i,r_i,s'_i\\right)\\right\\}_{i\\in [1,B]}$ (either by sampling from a larger training set, or directly from the system to control), with $(s,a) \\sim \\rho(\\cdot)$ and $s',a' \\sim p(s' | s,a)\\pi(a'|s')$, then one can derive a stochastic gradient descent learning procedure and iteratively learn $\\theta_{n+1}$ as the limit of the sequence $\\theta_{k+1} \\leftarrow \\theta_{k} + \\alpha_k d_n(\\theta_{k})$ with \n",
    "$$d_n(\\theta) = \\frac{1}{B} \\sum_{i=1}^B \\left[ \\left( r_i + \\gamma q(s_i',a';\\theta_{n}) - q(s_i,a_i;\\theta) \\right) \\nabla_\\theta q(s_i,a_i;\\theta) \\right].$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a2d8f-6f19-4e51-8a33-9f85ce181057",
   "metadata": {},
   "source": [
    "Taking $B=1$ and $(s,a)$ as the current state and action in the previous algorithm yields the historical TD(0) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be7625e-24b1-4d74-b11c-5218ea8f7188",
   "metadata": {},
   "source": [
    "# Learning optimal value functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c65fa5-2f57-4489-8bd6-be8505ef127d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Approximate value iteration as a sequence of risk minimization problems**  \n",
    "$$\\pi_n \\in \\mathcal{G} q_n,$$\n",
    "$$L_n(\\theta) = \\frac{1}{2} \\mathbb{E}_{(s,a) \\sim \\rho}\\left[ \\left( q(s,a;\\theta) - G^{\\pi_n}_1(s,a,q_n) \\right)^2 \\right],$$\n",
    "$$\\theta_{n+1} \\in \\arg\\min_{\\theta} L_n(\\theta),$$\n",
    "$$q_{n+1}(s,a) = q(s,a;\\theta_{n+1}).$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe69f97-74c1-4d8a-9adc-6dd7e40382d9",
   "metadata": {},
   "source": [
    "Minimizing $L_n$ with a single step of stochastic approximation: Q-learning.  \n",
    "Minimizing $L_n$ with SGD on a replay buffer (with a target network): DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6a162-847e-4b66-a976-01395d5e32b1",
   "metadata": {},
   "source": [
    "# Direct policy optimization\n",
    "\n",
    "TODO: the PG, the REINFORCE PG, the PG theorem, the DPG theorem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125cbc0f-545a-49b6-b56e-07e9ecc2b2b1",
   "metadata": {},
   "source": [
    "# The Bellman equation as a linear program"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
